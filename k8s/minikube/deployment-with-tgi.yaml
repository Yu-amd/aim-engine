apiVersion: apps/v1
kind: Deployment
metadata:
  name: aim-engine
  namespace: aim-engine
  labels:
    app: aim-engine
    version: v1
    component: deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: aim-engine
  template:
    metadata:
      labels:
        app: aim-engine
        version: v1
        component: deployment
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      # Security context for development
      securityContext:
        runAsUser: 0
        runAsGroup: 0
        fsGroup: 0
      
      # Wait for recipe selection to complete
      initContainers:
      - name: recipe-wait
        image: busybox:1.35
        command: ['sh', '-c']
        args:
        - |
          echo "Waiting for recipe selection to complete..."
          while ! kubectl get job aim-engine-recipe-selector-hook -o jsonpath='{.status.succeeded}' | grep -q "1"; do
            echo "Recipe selection job not completed yet, waiting..."
            sleep 10
          done
          echo "Recipe selection completed!"
        env:
        - name: KUBECONFIG
          value: "/root/.kube/config"
        volumeMounts:
        - name: kubeconfig
          mountPath: /root/.kube
          readOnly: true
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
      
      volumes:
      - name: kubeconfig
        secret:
          secretName: aim-engine-kubeconfig
          defaultMode: 0400
      
      containers:
      - name: aim-engine
        image: aim-tgi:latest
        imagePullPolicy: IfNotPresent
        
        # Port configuration
        ports:
        - name: http
          containerPort: 8000
          protocol: TCP
        - name: metrics
          containerPort: 8000
          protocol: TCP
        
        # Environment variables from ConfigMap
        env:
        - name: AIM_CACHE_DIR
          valueFrom:
            configMapKeyRef:
              name: aim-engine-config
              key: aim_cache_dir
        - name: AIM_CACHE_ENABLED
          valueFrom:
            configMapKeyRef:
              name: aim-engine-config
              key: aim_cache_enabled
        - name: HF_HOME
          valueFrom:
            configMapKeyRef:
              name: aim-engine-config
              key: hf_home
        - name: TRANSFORMERS_CACHE
          valueFrom:
            configMapKeyRef:
              name: aim-engine-config
              key: transformers_cache
        - name: HF_DATASETS_CACHE
          valueFrom:
            configMapKeyRef:
              name: aim-engine-config
              key: hf_datasets_cache
        - name: VLLM_CACHE_DIR
          valueFrom:
            configMapKeyRef:
              name: aim-engine-config
              key: vllm_cache_dir
        - name: HF_HUB_DISABLE_TELEMETRY
          valueFrom:
            configMapKeyRef:
              name: aim-engine-config
              key: hf_hub_disable_telemetry
        - name: HIP_VISIBLE_DEVICES
          valueFrom:
            configMapKeyRef:
              name: aim-engine-config
              key: hip_visible_devices
        - name: PYTORCH_ROCM_ARCH
          valueFrom:
            configMapKeyRef:
              name: aim-engine-config
              key: pytorch_rocm_arch
        - name: VLLM_USE_ROCM
          valueFrom:
            configMapKeyRef:
              name: aim-engine-config
              key: vllm_use_rocm
        - name: PYTORCH_CUDA_ALLOC_CONF
          valueFrom:
            configMapKeyRef:
              name: aim-engine-config
              key: pytorch_cuda_alloc_conf
        - name: PYTHONUNBUFFERED
          value: "1"
        - name: ENVIRONMENT
          value: "development"
        - name: LOG_LEVEL
          value: "DEBUG"
        # Recipe selection environment variables
        - name: RECIPE_ID
          valueFrom:
            configMapKeyRef:
              name: aim-engine-recipe-config
              key: RECIPE_ID
              optional: true
        - name: GPU_COUNT
          valueFrom:
            configMapKeyRef:
              name: aim-engine-recipe-config
              key: GPU_COUNT
              optional: true
        - name: PRECISION
          valueFrom:
            configMapKeyRef:
              name: aim-engine-recipe-config
              key: PRECISION
              optional: true
        - name: MODEL_ID
          valueFrom:
            configMapKeyRef:
              name: aim-engine-recipe-config
              key: MODEL_ID
              optional: true
        - name: BACKEND
          valueFrom:
            configMapKeyRef:
              name: aim-engine-recipe-config
              key: BACKEND
              optional: true
        
        # Increased resource limits for TGI
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"
        
        # Volume mounts
        volumeMounts:
        - name: model-cache
          mountPath: /workspace/model-cache
        - name: recipes
          mountPath: /workspace/aim-engine/recipes
          readOnly: true
        
        # Health checks for TGI server
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        # Startup probe for slow-starting TGI
        startupProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 60
        
        # Command to start TGI server with recipe configuration
        command: ["python3"]
        args: 
        - "-c"
        - |
          import os
          import sys
          import json
          import subprocess
          import time
          from pathlib import Path
          
          # Check if recipe configuration is available
          recipe_id = os.environ.get('RECIPE_ID')
          gpu_count = os.environ.get('GPU_COUNT')
          precision = os.environ.get('PRECISION')
          model_id = os.environ.get('MODEL_ID')
          backend = os.environ.get('BACKEND', 'tgi')
          
          print(f"üöÄ Starting AIM Engine with TGI backend...")
          print(f"   Recipe ID: {recipe_id}")
          print(f"   Model: {model_id}")
          print(f"   Backend: {backend}")
          
          if recipe_id and model_id:
              print(f"üéØ Using recipe configuration: {recipe_id}")
              
              # Start TGI server with recipe configuration
              tgi_cmd = [
                  "text-generation-launcher",
                  "--model-id", model_id,
                  "--port", "8000",
                  "--hostname", "0.0.0.0"
              ]
              
              # Add precision if specified
              if precision and precision != "null":
                  tgi_cmd.extend(["--dtype", precision])
              
              # Add additional TGI options for development
              tgi_cmd.extend([
                  "--max-batch-total-tokens", "8192",
                  "--max-batch-prefill-tokens", "4096",
                  "--max-input-length", "4096",
                  "--max-total-tokens", "8192"
              ])
              
              print(f"üìã TGI Command: {' '.join(tgi_cmd)}")
              
              # Start TGI server
              try:
                  subprocess.run(tgi_cmd, check=True)
              except subprocess.CalledProcessError as e:
                  print(f"‚ùå TGI server failed to start: {e}")
                  print("üîÑ Falling back to mock server...")
                  
                  # Fallback to mock server
                  from http.server import HTTPServer, BaseHTTPRequestHandler
                  
                  class MockTGIHandler(BaseHTTPRequestHandler):
                      def do_GET(self):
                          if self.path == '/health':
                              self.send_response(200)
                              self.send_header('Content-type', 'application/json')
                              self.end_headers()
                              response = {
                                  "status": "healthy",
                                  "recipe_id": recipe_id,
                                  "model_id": model_id,
                                  "backend": "tgi-mock",
                                  "environment": "development"
                              }
                              self.wfile.write(json.dumps(response).encode())
                          elif self.path == '/metrics':
                              self.send_response(200)
                              self.send_header('Content-type', 'text/plain')
                              self.end_headers()
                              metrics = f"""
# HELP aim_recipe_selection_total Total number of recipe selections
# TYPE aim_recipe_selection_total counter
aim_recipe_selection_total{{recipe_id="{recipe_id}",model="{model_id}",backend="tgi-mock"}} 1

# HELP aim_performance_tokens_per_second Tokens per second (mock)
# TYPE aim_performance_tokens_per_second gauge
aim_performance_tokens_per_second{{recipe_id="{recipe_id}"}} 120.5
                              """
                              self.wfile.write(metrics.encode())
                          else:
                              self.send_response(404)
                              self.end_headers()
                              self.wfile.write(b"Not Found")
                      
                      def log_message(self, format, *args):
                          pass
                  
                  server = HTTPServer(('0.0.0.0', 8000), MockTGIHandler)
                  print(f"üåê Mock TGI server started on port 8000")
                  server.serve_forever()
          else:
              print("‚ö†Ô∏è  No recipe configuration found, starting basic TGI server")
              
              # Start basic TGI server
              basic_cmd = [
                  "text-generation-launcher",
                  "--model-id", "microsoft/DialoGPT-medium",  # Small model for testing
                  "--port", "8000",
                  "--hostname", "0.0.0.0",
                  "--max-batch-total-tokens", "4096"
              ]
              
              print(f"üìã Basic TGI Command: {' '.join(basic_cmd)}")
              subprocess.run(basic_cmd, check=True)
      
      # Volumes
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: aim-engine-cache-pvc
      - name: recipes
        configMap:
          name: aim-engine-recipes
      
      # Restart policy
      restartPolicy: Always 