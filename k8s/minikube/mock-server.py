#!/usr/bin/env python3
"""
Mock AIM Engine API Server for Minikube Development
This server simulates the AIM Engine API endpoints for testing Kubernetes deployment
"""

import json
import time
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import urlparse, parse_qs
import threading

class MockAIMEngineHandler(BaseHTTPRequestHandler):
    def __init__(self, *args, **kwargs):
        self.routes = {
            '/health': self.health_check,
            '/v1/models': self.list_models,
            '/v1/completions': self.completions,
            '/v1/chat/completions': self.chat_completions,
            '/metrics': self.metrics,
            '/': self.root
        }
        super().__init__(*args, **kwargs)
    
    def do_GET(self):
        """Handle GET requests"""
        parsed_url = urlparse(self.path)
        path = parsed_url.path
        
        if path in self.routes:
            self.routes[path]()
        else:
            self.send_error(404, "Endpoint not found")
    
    def do_POST(self):
        """Handle POST requests"""
        parsed_url = urlparse(self.path)
        path = parsed_url.path
        
        if path in self.routes:
            self.routes[path]()
        else:
            self.send_error(404, "Endpoint not found")
    
    def health_check(self):
        """Health check endpoint"""
        self.send_response(200)
        self.send_header('Content-type', 'application/json')
        self.end_headers()
        response = {
            "status": "healthy",
            "timestamp": time.time(),
            "service": "aim-engine-mock",
            "version": "1.0.0"
        }
        self.wfile.write(json.dumps(response).encode())
    
    def root(self):
        """Root endpoint"""
        self.send_response(200)
        self.send_header('Content-type', 'application/json')
        self.end_headers()
        response = {
            "message": "AIM Engine Mock API Server",
            "status": "running",
            "endpoints": [
                "/health",
                "/v1/models",
                "/v1/completions",
                "/v1/chat/completions",
                "/metrics"
            ]
        }
        self.wfile.write(json.dumps(response, indent=2).encode())
    
    def list_models(self):
        """List available models"""
        self.send_response(200)
        self.send_header('Content-type', 'application/json')
        self.end_headers()
        response = {
            "object": "list",
            "data": [
                {
                    "id": "Qwen/Qwen3-32B",
                    "object": "model",
                    "created": int(time.time()),
                    "owned_by": "aim-engine"
                }
            ]
        }
        self.wfile.write(json.dumps(response).encode())
    
    def completions(self):
        """Handle completion requests"""
        content_length = int(self.headers.get('Content-Length', 0))
        if content_length > 0:
            post_data = self.rfile.read(content_length)
            try:
                request_data = json.loads(post_data.decode('utf-8'))
                prompt = request_data.get('prompt', 'Hello, how are you?')
                max_tokens = request_data.get('max_tokens', 50)
            except json.JSONDecodeError:
                prompt = 'Hello, how are you?'
                max_tokens = 50
        else:
            prompt = 'Hello, how are you?'
            max_tokens = 50
        
        # Generate mock response
        mock_response = f"This is a mock response to: '{prompt}'. In a real deployment, this would be generated by the AIM Engine with vLLM."
        
        self.send_response(200)
        self.send_header('Content-type', 'application/json')
        self.end_headers()
        response = {
            "id": f"mock-{int(time.time())}",
            "object": "text_completion",
            "created": int(time.time()),
            "model": "Qwen/Qwen3-32B",
            "choices": [
                {
                    "text": mock_response,
                    "index": 0,
                    "logprobs": None,
                    "finish_reason": "length"
                }
            ],
            "usage": {
                "prompt_tokens": len(prompt.split()),
                "completion_tokens": len(mock_response.split()),
                "total_tokens": len(prompt.split()) + len(mock_response.split())
            }
        }
        self.wfile.write(json.dumps(response).encode())
    
    def chat_completions(self):
        """Handle chat completion requests"""
        content_length = int(self.headers.get('Content-Length', 0))
        if content_length > 0:
            post_data = self.rfile.read(content_length)
            try:
                request_data = json.loads(post_data.decode('utf-8'))
                messages = request_data.get('messages', [])
                max_tokens = request_data.get('max_tokens', 100)
            except json.JSONDecodeError:
                messages = [{"role": "user", "content": "Hello, how are you?"}]
                max_tokens = 100
        else:
            messages = [{"role": "user", "content": "Hello, how are you?"}]
            max_tokens = 100
        
        # Extract user message
        user_message = ""
        for msg in messages:
            if msg.get('role') == 'user':
                user_message = msg.get('content', '')
                break
        
        # Generate mock response
        mock_response = f"This is a mock chat response to: '{user_message}'. In a real deployment, this would be generated by the AIM Engine with vLLM."
        
        self.send_response(200)
        self.send_header('Content-type', 'application/json')
        self.end_headers()
        response = {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": "Qwen/Qwen3-32B",
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": mock_response
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": len(user_message.split()),
                "completion_tokens": len(mock_response.split()),
                "total_tokens": len(user_message.split()) + len(mock_response.split())
            }
        }
        self.wfile.write(json.dumps(response).encode())
    
    def metrics(self):
        """Metrics endpoint"""
        self.send_response(200)
        self.send_header('Content-type', 'text/plain')
        self.end_headers()
        metrics = f"""# HELP aim_engine_requests_total Total number of requests
# TYPE aim_engine_requests_total counter
aim_engine_requests_total{{endpoint="/health"}} 1
aim_engine_requests_total{{endpoint="/v1/models"}} 1
aim_engine_requests_total{{endpoint="/v1/completions"}} 1

# HELP aim_engine_response_time_seconds Response time in seconds
# TYPE aim_engine_response_time_seconds histogram
aim_engine_response_time_seconds_bucket{{le="0.1"}} 1
aim_engine_response_time_seconds_bucket{{le="0.5"}} 1
aim_engine_response_time_seconds_bucket{{le="1.0"}} 1
aim_engine_response_time_seconds_bucket{{le="+Inf"}} 1

# HELP aim_engine_up AIM Engine service status
# TYPE aim_engine_up gauge
aim_engine_up 1
"""
        self.wfile.write(metrics.encode())
    
    def log_message(self, format, *args):
        """Override to reduce logging noise"""
        pass

def run_server(port=8000):
    """Run the mock server"""
    server_address = ('', port)
    httpd = HTTPServer(server_address, MockAIMEngineHandler)
    print(f"AIM Engine Mock API Server starting on port {port}...")
    print("Available endpoints:")
    print("  GET  /health - Health check")
    print("  GET  /v1/models - List models")
    print("  POST /v1/completions - Text completion")
    print("  POST /v1/chat/completions - Chat completion")
    print("  GET  /metrics - Prometheus metrics")
    print("  GET  / - Root endpoint")
    httpd.serve_forever()

if __name__ == '__main__':
    run_server() 