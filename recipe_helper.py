#!/usr/bin/env python3
"""
Recipe Helper - Uses AIM Engine to select optimal recipes and generate vLLM commands
"""

import sys
import json
from pathlib import Path

# Add current directory to path so we can import AIM modules
sys.path.insert(0, str(Path(__file__).parent))

from aim_recipe_selector import AIMRecipeSelector
from aim_config_generator import AIMConfigGenerator

def get_optimal_vllm_command(model_id: str, gpu_count: int = None, precision: str = None, backend: str = 'vllm'):
    """
    Get the optimal vLLM command using AIM Engine's recipe selection
    
    Args:
        model_id: Hugging Face model ID (e.g., "Qwen/Qwen3-32B")
        gpu_count: Number of GPUs (optional - will auto-detect)
        precision: Precision format (optional - will auto-select)
        backend: Serving backend (default: 'vllm')
    
    Returns:
        Dictionary with configuration and vLLM command
    """
    try:
        # Initialize AIM components
        recipe_selector = AIMRecipeSelector(".")
        config_generator = AIMConfigGenerator()
        
        print(f"üîç Analyzing model: {model_id}")
        if gpu_count:
            print(f"üìä Customer specified GPU count: {gpu_count}")
        if precision:
            print(f"üéØ Customer specified precision: {precision}")
        print(f"‚öôÔ∏è  Backend: {backend}")
        
        # Step 1: Get optimal configuration
        optimal_config = recipe_selector.get_optimal_configuration(
            model_id, gpu_count, precision, backend
        )
        
        if not optimal_config:
            print(f"‚ùå No suitable configuration found for {model_id}")
            return None
        
        print(f"\n‚úÖ Selected configuration:")
        print(f"   Recipe: {optimal_config['recipe_id']}")
        print(f"   GPU Count: {optimal_config['gpu_count']} (available: {optimal_config['available_gpus']})")
        print(f"   Precision: {optimal_config['precision']}")
        print(f"   Backend: {optimal_config['backend']}")
        
        # Step 2: Get the full recipe
        recipe = recipe_selector.get_recipe_info(optimal_config["recipe_id"])
        if not recipe:
            print(f"‚ùå Recipe {optimal_config['recipe_id']} not found")
            return None
        
        # Step 3: Generate deployment configuration
        config = config_generator.generate_config(
            recipe, 
            optimal_config['gpu_count'], 
            optimal_config['precision'], 
            optimal_config['backend'], 
            8000  # Default port
        )
        
        # Step 4: Extract vLLM command
        vllm_command = config.get("command", "")
        
        # Step 5: Build the full Docker command
        docker_command = f"""docker run --rm \\
  --device=/dev/kfd \\
  --device=/dev/dri \\
  --group-add=video \\
  --group-add=render \\
  -v /workspace/model-cache:/workspace/model-cache \\
  -p 8000:8000 \\
  rocm/vllm:latest \\
  {vllm_command}"""
        
        result = {
            "model_id": model_id,
            "recipe_id": optimal_config["recipe_id"],
            "gpu_count": optimal_config["gpu_count"],
            "available_gpus": optimal_config["available_gpus"],
            "precision": optimal_config["precision"],
            "backend": optimal_config["backend"],
            "vllm_command": vllm_command,
            "docker_command": docker_command,
            "config": config
        }
        
        return result
        
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        return None

def main():
    """Main function for command-line usage"""
    if len(sys.argv) < 2:
        print("Usage: python recipe_helper.py <model_id> [gpu_count] [precision]")
        print("Examples:")
        print("  python recipe_helper.py Qwen/Qwen3-32B")
        print("  python recipe_helper.py Qwen/Qwen3-32B 4")
        print("  python recipe_helper.py Qwen/Qwen3-32B 4 bf16")
        sys.exit(1)
    
    model_id = sys.argv[1]
    gpu_count = int(sys.argv[2]) if len(sys.argv) > 2 else None
    precision = sys.argv[3] if len(sys.argv) > 3 else None
    
    result = get_optimal_vllm_command(model_id, gpu_count, precision)
    
    if result:
        print(f"\nüöÄ Generated vLLM Command:")
        print(f"\n{result['docker_command']}")
        
        print(f"\nüìã Configuration Summary:")
        print(f"   Model: {result['model_id']}")
        print(f"   Recipe: {result['recipe_id']}")
        print(f"   GPUs: {result['gpu_count']} (from {result['available_gpus']} available)")
        print(f"   Precision: {result['precision']}")
        print(f"   Backend: {result['backend']}")
        
        # Save to file for easy access
        with open("vllm_command.sh", "w") as f:
            f.write("#!/bin/bash\n")
            f.write(f"# Generated by AIM Engine for {result['model_id']}\n")
            f.write(f"# Recipe: {result['recipe_id']}\n\n")
            f.write(result['docker_command'])
        
        print(f"\nüíæ Command saved to: vllm_command.sh")
        print(f"   Run: chmod +x vllm_command.sh && ./vllm_command.sh")
    else:
        sys.exit(1)

if __name__ == "__main__":
    main() 