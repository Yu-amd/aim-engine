# AIM Engine Design Document

---

## üìã **Table of Contents**

1. [Executive Summary](#executive-summary)
2. [Architecture Overview](#architecture-overview)
3. [System Architecture](#system-architecture)
4. [Core Components](#core-components)
5. [Model Caching System](#model-caching-system)
6. [Recipe Selection Engine](#recipe-selection-engine)
7. [Performance Optimization](#performance-optimization)
8. [Deployment Models](#deployment-models)
9. [Roadmap](#roadmap)
10. [API Reference](#api-reference)
11. [Troubleshooting](#troubleshooting)

---

## üéØ **Executive Summary**

AMD Inference Microservice is the AMD equivalent of NVIDIA's NIM ecosystem. Just as NVIDIA NIM Operator manages and orchestrates NIM deployments on NVIDIA hardware, **AIM Engine** ensures efficient and successful deployment of AIM (AMD Inference Microservices) on AMD hardware.

**AIM Engine** is an intelligent AI model deployment system that automatically optimizes large language model serving on AMD hardware. It combines intelligent recipe selection, dynamic resource detection, and advanced caching to deliver optimal performance with zero configuration, serving as the essential dependency management and orchestration layer for AIM deployments.

### **Key Value Propositions**
- **AMD Engine**: Direct counterpart to NVIDIA NIM Operator for AMD hardware
- **Zero Configuration**: Works out-of-the-box with automatic optimization
- **Hardware Intelligence**: AMD GPU-aware optimization and resource allocation
- **Performance Optimization**: Up to 10x faster deployment with intelligent caching
- **Enterprise Ready**: Kubernetes, Helm, and KServe integration roadmap
- **Streaming Support**: Real-time response streaming for enhanced UX
- **Dependency Management**: Ensures all AIM deployment dependencies are met

### **Technical Highlights**
- **AMD Engine**: Direct equivalent to NVIDIA NIM Operator for AMD ecosystem
- **Unified Container**: AIM Engine tools integrated into vLLM ROCm container
- **Intelligent Caching**: Persistent model caching with significant speed improvement
- **Dynamic Optimization**: Model-size based GPU allocation and precision selection
- **Multi-Backend Support**: vLLM, SGLang (roadmap), and extensible backend architecture
- **Production Ready**: Comprehensive monitoring, logging, and health checks
- **Dependency Orchestration**: Manages all AIM deployment prerequisites and requirements

---

## üèóÔ∏è **Architecture Overview**

### **AIM Engine vs NVIDIA NIM Operator Comparison**

| Aspect            | NVIDIA NIM Operator                        | AMD AIM Engine                          |
|-------------------|--------------------------------------------|-----------------------------------------|
| **Purpose**       | Manages NIM deployments on NVIDIA hardware | Manages AIM deployments on AMD hardware |
| **Hardware**      | NVIDIA GPUs (A100, H100, etc.)             | AMD GPUs (MI300X, MI325X, etc.)         |
| **Runtime**       | NVIDIA CUDA/TRT                            | AMD ROCm/vLLM                           |
| **Orchestration** | Kubernetes Operator                        | Kubernetes Operator (Roadmap)           |
| **Optimization**  | NVIDIA-specific tuning                     | AMD-specific tuning                     |
| **Caching**       | Model caching for NIM                      | Model caching for AIM                   |
| **Dependencies**  | CUDA drivers, TRT                          | ROCm drivers, vLLM                      |

### **High-Level Architecture**

```mermaid
graph TB
    subgraph "User Interface Layer"
        A[CLI Commands] --> B[Web Interface]
        C[API Endpoints] --> B
    end
    
    subgraph "AIM Engine Core"
        D[Recipe Selector] --> E[Config Generator]
        F[Cache Manager] --> E
        G[Resource Detector] --> D
    end
    
    subgraph "Backend Runtime"
        H[vLLM Runtime] --> I[Model Serving]
        J[SGLang Runtime] --> I
        K[Custom Backends] --> I
    end
    
    subgraph "Hardware Layer"
        L[AMD GPUs] --> M[ROCm Runtime]
        N[System Memory] --> M
        O[Storage Cache] --> M
    end
    
    B --> D
    E --> H
    E --> J
    H --> L
    J --> L
```

### **Container Architecture**

```mermaid
graph LR
    subgraph "AIM-vLLM Container"
        A[AIM Engine Tools] --> B[vLLM Runtime]
        C[Model Cache] --> B
        D[Recipe Database] --> A
        E[Resource Monitor] --> A
    end
    
    subgraph "Host System"
        F[AMD GPUs] --> G[ROCm Drivers]
        H[Model Storage] --> I[Cache Volume]
    end
    
    B --> G
    C --> I
```

---

## üîß **System Architecture**

### **AIM Engine**

**AIM Engine** serves as the AMD equivalent of NVIDIA's NIM Operator, providing the essential orchestration and dependency management layer for AIM (AMD Inference Microservice) deployments. Just as NIM Operator ensures successful NIM deployments on NVIDIA hardware, AIM Engine guarantees efficient and optimized AIM deployments on AMD hardware.

### **Component Architecture**

```mermaid
graph TD
    subgraph "AIM Engine Core"
        A[AIMRecipeSelector] --> B[AIMConfigGenerator]
        C[AIMCacheManager] --> B
        D[ResourceDetector] --> A
    end
    
    subgraph "Data Layer"
        E[Models Directory] --> A
        F[Recipes Directory] --> A
        G[Cache Directory] --> C
    end
    
    subgraph "Runtime Layer"
        H[vLLM Backend] --> I[Model Serving]
        J[SGLang Backend] --> I
        K[Custom Backends] --> I
    end
    
    subgraph "Hardware Layer"
        L[AMD GPUs] --> M[ROCm Runtime]
        N[System Memory] --> M
        O[Storage] --> G
    end
    
    B --> H
    B --> J
    H --> L
    J --> L
```

### **Data Flow Architecture**

```mermaid
sequenceDiagram
    participant U as User
    participant A as AIM Engine
    participant C as Cache Manager
    participant R as Recipe Selector
    participant V as vLLM Runtime
    participant G as GPU Hardware
    
    U->>A: Deploy Model Request
    A->>C: Check Cache Status
    C-->>A: Cache Hit/Miss
    
    alt Cache Miss
        A->>C: Download & Cache Model
        C-->>A: Model Cached
    end
    
    A->>R: Select Optimal Recipe
    R->>R: Detect GPU Resources
    R-->>A: Optimal Configuration
    
    A->>V: Generate vLLM Command
    V->>G: Initialize GPU Resources
    G-->>V: GPU Ready
    V-->>A: Server Started
    A-->>U: Deployment Complete
```

---

## üß© **Core Components**

### **1. AIMRecipeSelector**

**Purpose**: Intelligent recipe selection and resource optimization (AMD equivalent of NIM Controller)

**Key Features**:
- Multi-level GPU detection (vLLM ‚Üí Container ‚Üí Host)
- Model-size based optimization
- Dynamic precision selection
- Fallback strategy management

**Architecture**:
```python
class AIMRecipeSelector:
    def __init__(self):
        self.models = self._load_models()
        self.recipes = self._load_recipes()
    
    def get_optimal_configuration(self, model_id, gpu_count=None, precision=None):
        # 1. Resource Detection
        available_gpus = self._detect_gpus()
        
        # 2. Model Analysis
        model_info = self.models.get(model_id, {})
        model_size = model_info.get('size', 'unknown')
        
        # 3. Optimization
        optimal_gpus = self._optimize_gpu_count(model_size, available_gpus, gpu_count)
        optimal_precision = self._optimize_precision(model_size, precision)
        
        # 4. Recipe Selection
        recipe = self._select_recipe(model_id, optimal_gpus, optimal_precision)
        
        return recipe
```

### **2. AIMConfigGenerator**

**Purpose**: Generates deployment configurations from selected recipes (AMD equivalent of NIM Config Manager)

**Key Features**:
- vLLM command generation
- Environment variable setup
- Docker configuration creation
- Resource allocation optimization

**Architecture**:
```python
class AIMConfigGenerator:
    def generate_deployment_config(self, recipe, model_id, port=8000):
        # 1. Command Generation
        vllm_command = self._build_vllm_command(recipe, port)
        
        # 2. Environment Setup
        env_vars = self._build_environment(recipe)
        
        # 3. Resource Allocation
        resource_config = self._allocate_resources(recipe)
        
        # 4. Cache Integration
        cache_config = self._integrate_cache(model_id)
        
        return {
            'command': vllm_command,
            'environment': env_vars,
            'resources': resource_config,
            'cache': cache_config
        }
```

### **3. AIMCacheManager**

**Purpose**: Intelligent model caching and storage management (AMD equivalent of NIM Cache Manager)

**Key Features**:
- Persistent model storage
- Cache metadata tracking
- Automatic cleanup
- Volume mount configuration

**Architecture**:
```python
class AIMCacheManager:
    def __init__(self, cache_dir="/workspace/model-cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_index = self._load_cache_index()
    
    def is_model_cached(self, model_id):
        return self.cache_index.get(model_id, {}).get('cached', False)
    
    def add_model_to_cache(self, model_id, model_path):
        # Copy model files
        cache_path = self._get_cache_path(model_id)
        shutil.copytree(model_path, cache_path)
        
        # Update metadata
        self._update_cache_index(model_id, cache_path)
    
    def get_cache_stats(self):
        return {
            'total_models': len(self.list_cached_models()),
            'total_size_gb': self._calculate_total_size(),
            'cache_hit_rate': self._calculate_hit_rate()
        }
```

---

## üóÑÔ∏è **Model Caching System**

### **AIM Caching**

Just as NVIDIA NIM Operator provides intelligent caching for NIM deployments, **AIM Engine** delivers comprehensive caching for AIM deployments. The caching system ensures that model dependencies are efficiently managed and reused across deployments, providing the same level of optimization that NIM users expect.

### **Cache Architecture**

```mermaid
graph TB
    subgraph "Cache Manager"
        A[Cache Index] --> B[Cache Operations]
        C[Cache Statistics] --> B
        D[Cache Cleanup] --> B
    end
    
    subgraph "Cache Storage"
        E[Model Files] --> F[Tokenizer Files]
        G[Config Files] --> F
        H[Dataset Files] --> F
    end
    
    subgraph "Cache Metadata"
        I[Model Info] --> J[Size Tracking]
        K[Version Control] --> J
        L[Access Patterns] --> J
    end
    
    B --> E
    B --> I
    E --> J
```

### **Cache Performance Benefits**

| Metric | First Deployment | Cached Deployment | Improvement |
|--------|------------------|-------------------|-------------|
| **Download Time** | 30-60 minutes | 0 minutes | ‚àû |
| **Setup Time** | 5-10 minutes | 30 seconds | 10-20x |
| **Network Usage** | 10-100 GB | 0 GB | 100% |
| **Reliability** | Network dependent | Local only | 100% |

### **Cache Management Operations**

```python
# Cache Statistics
cache_stats = {
    "total_models": 15,
    "total_size_gb": 245.7,
    "cache_hit_rate": 0.87,
    "avg_deployment_time": "2.3s",
    "storage_efficiency": 0.92
}

# Cache Operations
cache_manager.add_model_to_cache("Qwen/Qwen3-32B", model_path)
cache_manager.remove_model_from_cache("old-model")
cache_manager.cleanup_old_models(days_old=30)
```

---

## üéØ **Recipe Selection Engine**

### **AIM Recipe Selection**

Similar to how NVIDIA NIM Operator optimizes NIM deployments for NVIDIA hardware, **AIM Engine** provides AMD-specific optimization through intelligent recipe selection. The recipe engine ensures that AIM deployments are configured optimally for AMD GPUs, delivering the same level of performance optimization that NIM users experience.

### **Recipe Selection Algorithm**

```mermaid
flowchart TD
    A[Model ID Input] --> B[Resource Detection]
    B --> C[Model Analysis]
    C --> D[GPU Count Optimization]
    D --> E[Precision Selection]
    E --> F[Recipe Matching]
    F --> G{Recipe Found?}
    G -->|Yes| H[Return Recipe]
    G -->|No| I[Fallback Strategy]
    I --> J[Try Lower GPU Count]
    J --> K[Try Different Precision]
    K --> L[Try Alternative Backend]
    L --> F
```

### **Optimization Strategies**

#### **Model-Size Based Optimization**

| Model Size | Optimal GPUs | Precision | Memory Utilization | Batch Tokens |
|------------|--------------|-----------|-------------------|--------------|
| **7B-8B** | 1 | fp16 | 85% | 8,192 |
| **13B-14B** | 2 | bf16 | 90% | 16,384 |
| **32B-34B** | 4 | bf16 | 90% | 32,768 |
| **70B+** | 8 | bf16 | 90% | 65,536 |

#### **Hardware-Specific Optimization**

```yaml
# MI300X Optimization
mi300x_config:
  gpu_memory_utilization: 0.9
  max_model_len: 32768
  tensor_parallel_size: auto
  dtype: bfloat16

# MI325X Optimization  
mi325x_config:
  gpu_memory_utilization: 0.95
  max_model_len: 65536
  tensor_parallel_size: auto
  dtype: bfloat16
```

### **Recipe Structure**

```yaml
recipe_id: qwen3-32b-mi300x-bf16
model_id: Qwen/Qwen3-32B
hardware: MI300X
precision: bf16
readiness_level: production

vllm_serve:
  1_gpu:
    enabled: true
    args:
      --model: Qwen/Qwen3-32B
      --dtype: bfloat16
      --max-num-batched-tokens: 8192
      --max-model-len: 32768
      --gpu-memory-utilization: 0.9
      --trust-remote-code: true
  
  2_gpu:
    enabled: true
    args:
      --tensor-parallel-size: 2
      --max-num-batched-tokens: 16384
      --max-model-len: 32768
      --gpu-memory-utilization: 0.9
      --trust-remote-code: true
  
  4_gpu:
    enabled: true
    args:
      --tensor-parallel-size: 4
      --max-num-batched-tokens: 32768
      --max-model-len: 32768
      --gpu-memory-utilization: 0.9
      --trust-remote-code: true
```

---

## ‚ö° **Performance Optimization**

### **Optimization Techniques**

1. **Dynamic Batching**: Automatic batch size adjustment based on GPU count
2. **Tensor Parallelism**: Automatic scaling with GPU count
3. **Memory Pinning**: Optimized memory allocation for AMD GPUs
4. **Precision Selection**: Hardware-aware precision choices
5. **Cache Optimization**: Intelligent model caching and prefetching

---

## üöÄ **Deployment Models**

### **Current Deployment: Container-Based**

```bash
# Single Container Deployment
docker run --rm -it \
  --device=/dev/kfd \
  --device=/dev/dri \
  --group-add=video \
  --group-add=render \
  -v /workspace/model-cache:/workspace/model-cache \
  -p 8000:8000 \
  aim-vllm:latest \
  aim-generate Qwen/Qwen3-32B
```

### **Future Deployment: Kubernetes**

```yaml
# Kubernetes Deployment (Roadmap)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: aim-engine-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: aim-engine
  template:
    metadata:
      labels:
        app: aim-engine
    spec:
      containers:
      - name: aim-engine
        image: aim-vllm:latest
        ports:
        - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 4
        volumeMounts:
        - name: model-cache
          mountPath: /workspace/model-cache
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: model-cache-pvc
```

---

## üó∫Ô∏è **Roadmap**

### **Phase 1: Q1 2024 - Enterprise Foundation**

#### **Kubernetes Integration**
- [ ] **Helm Charts**: Complete Helm chart for AIM Engine deployment
- [ ] **KServe Integration**: Native KServe backend support
- [ ] **Multi-Node Support**: Distributed deployment across multiple nodes
- [ ] **Resource Management**: Kubernetes resource quotas and limits

#### **Enhanced Monitoring**
- [ ] **Prometheus Metrics**: Comprehensive metrics collection
- [ ] **Grafana Dashboards**: Real-time performance monitoring
- [ ] **Health Checks**: Kubernetes liveness and readiness probes
- [ ] **Logging**: Structured logging with ELK stack integration

### **Phase 2: Q2 2024 - Advanced Features**

#### **Multi-Backend Support**
- [ ] **SGLang Integration**: Native SGLang backend support
- [ ] **Custom Backends**: Plugin architecture for custom backends
- [ ] **Backend Comparison**: Performance comparison tools
- [ ] **Dynamic Backend Switching**: Runtime backend selection

#### **Advanced Caching**
- [ ] **Distributed Caching**: Redis-based distributed cache
- [ ] **Cache Prefetching**: Intelligent model prefetching
- [ ] **Cache Compression**: Model compression for storage efficiency
- [ ] **Cache Analytics**: Advanced cache performance analytics

### **Phase 3: Q3 2024 - Production Features**

#### **High Availability**
- [ ] **Auto-scaling**: Horizontal Pod Autoscaler (HPA) support
- [ ] **Load Balancing**: Intelligent load balancing across replicas
- [ ] **Fault Tolerance**: Automatic failover and recovery
- [ ] **Disaster Recovery**: Backup and restore capabilities

#### **Security & Compliance**
- [ ] **RBAC Integration**: Role-based access control
- [ ] **Network Policies**: Kubernetes network policies
- [ ] **Secrets Management**: Secure credential management
- [ ] **Audit Logging**: Comprehensive audit trails

### **Phase 4: Q4 2024 - Enterprise Features**

#### **Multi-Tenancy**
- [ ] **Namespace Isolation**: Multi-tenant deployment support
- [ ] **Resource Quotas**: Per-tenant resource limits
- [ ] **Billing Integration**: Usage tracking and billing
- [ ] **Tenant Management**: Tenant lifecycle management

#### **Advanced Analytics**
- [ ] **Performance Analytics**: Advanced performance insights
- [ ] **Cost Optimization**: Cost analysis and optimization
- [ ] **Capacity Planning**: Predictive capacity planning
- [ ] **Business Intelligence**: BI dashboard integration

### **AIM Engine Kubernetes Operator**

The Kubernetes roadmap for AIM Engine mirrors the evolution of NVIDIA NIM Operator, providing the same level of enterprise-grade orchestration and management capabilities. This roadmap ensures that AIM Engine becomes the definitive AMD equivalent of NVIDIA NIM Operator in the Kubernetes ecosystem.

### **Detailed Kubernetes Roadmap**

#### **Helm Chart Development**

```yaml
# aim-engine/values.yaml
aimEngine:
  replicaCount: 3
  
  image:
    repository: aim-vllm
    tag: latest
    pullPolicy: IfNotPresent
  
  resources:
    limits:
      nvidia.com/gpu: 4
    requests:
      memory: "32Gi"
      cpu: "8"
  
  cache:
    enabled: true
    storageClass: "fast-ssd"
    size: "1Ti"
  
  monitoring:
    enabled: true
    prometheus: true
    grafana: true
  
  ingress:
    enabled: true
    className: "nginx"
    annotations:
      cert-manager.io/cluster-issuer: "letsencrypt-prod"
```

#### **KServe Integration**

```yaml
# kserve-aim-engine.yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: aim-engine-service
spec:
  predictor:
    containers:
    - name: aim-engine
      image: aim-vllm:latest
      ports:
      - containerPort: 8000
        protocol: TCP
      resources:
        limits:
          nvidia.com/gpu: 4
      volumeMounts:
      - name: model-cache
        mountPath: /workspace/model-cache
  volumes:
  - name: model-cache
    persistentVolumeClaim:
      claimName: model-cache-pvc
```

#### **Monitoring Stack**

```yaml
# monitoring-stack.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
    scrape_configs:
    - job_name: 'aim-engine'
      static_configs:
      - targets: ['aim-engine-service:8000']
      metrics_path: '/metrics'
```

### **Performance Targets**

| Metric | Current | Target (Q4 2024) | Improvement |
|--------|---------|------------------|-------------|
| **Deployment Time** | 30s | 5s | 6x |
| **Cache Hit Rate** | 87% | 95% | +8% |
| **GPU Utilization** | 91% | 95% | +4% |
| **Throughput** | 164 tokens/s | 200 tokens/s | +22% |
| **Availability** | 99.5% | 99.9% | +0.4% |

---

## üìö **API Reference**

### **CLI Commands**

```bash
# Generate deployment configuration
aim-generate <model_id> [--gpu-count N] [--precision fp16|bf16]

# Start vLLM server with generated config
aim-serve <model_id> [--port N]

# Interactive shell
aim-shell [command]

# Cache management
aim-cache list
aim-cache add <model_id>
aim-cache remove <model_id>
aim-cache stats
```

### **REST API Endpoints**

```python
# Health check
GET /health
Response: {"status": "healthy", "version": "1.0.0"}

# Model deployment
POST /deploy
Body: {"model_id": "Qwen/Qwen3-32B", "gpu_count": 4}
Response: {"deployment_id": "deploy_123", "status": "running"}

# Cache management
GET /cache/stats
Response: {"total_models": 15, "total_size_gb": 245.7}

# Performance metrics
GET /metrics
Response: Prometheus metrics format
```

### **Configuration Files**

#### **Model Configuration**
```yaml
# models/qwen3-32b.yaml
model_id: Qwen/Qwen3-32B
name: Qwen 3 32B
size: 32B
family: Qwen
readiness_level: production
description: Qwen 3 32B model optimized for AMD hardware
```

#### **Recipe Configuration**
```yaml
# recipes/qwen3-32b-mi300x.yaml
recipe_id: qwen3-32b-mi300x-bf16
model_id: Qwen/Qwen3-32B
hardware: MI300X
precision: bf16
readiness_level: production
vllm_serve:
  4_gpu:
    enabled: true
    args:
      --model: Qwen/Qwen3-32B
      --dtype: bfloat16
      --tensor-parallel-size: 4
      --max-num-batched-tokens: 32768
```

---

## üîß **Troubleshooting**

### **Common Issues**

#### **1. GPU Detection Issues**
```bash
# Check GPU availability
rocm-smi --showproductname
nvidia-smi  # For CUDA compatibility

# Verify ROCm installation
rocm-smi --version
```

#### **2. Cache Issues**
```bash
# Check cache status
aim-cache stats

# Clear corrupted cache
aim-cache remove <model_id>
aim-cache add <model_id>
```

#### **3. Performance Issues**
```bash
# Check GPU utilization
rocm-smi --showmeminfo

# Monitor system resources
htop
nvidia-smi  # For CUDA compatibility
```

### **Debug Commands**

```bash
# Enable debug logging
export AIM_DEBUG=1
aim-generate <model_id>

# Check container logs
docker logs <container_id>

# Verify configuration
aim-generate <model_id> --dry-run
```

### **Performance Tuning**

```bash
# Optimize for specific hardware
aim-generate <model_id> --hardware MI300X

# Custom precision
aim-generate <model_id> --precision bf16

# Manual GPU allocation
aim-generate <model_id> --gpu-count 4
```

---

## üìä **Metrics & Monitoring**

### **Key Performance Indicators**

| KPI | Target | Current | Status |
|-----|--------|---------|--------|
| **Deployment Speed** | < 10s | 30s | üü° Improving |
| **Cache Hit Rate** | > 90% | 87% | üü° Improving |
| **GPU Utilization** | > 90% | 91% | ‚úÖ Good |
| **Throughput** | > 150 tokens/s | 164 tokens/s | ‚úÖ Good |
| **Availability** | > 99.9% | 99.5% | üü° Improving |

### **Monitoring Dashboard**

```yaml
# grafana-dashboard.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard
data:
  dashboard.json: |
    {
      "dashboard": {
        "title": "AIM Engine Performance",
        "panels": [
          {
            "title": "Deployment Time",
            "type": "graph",
            "targets": [
              {
                "expr": "aim_deployment_time_seconds"
              }
            ]
          },
          {
            "title": "Cache Hit Rate",
            "type": "stat",
            "targets": [
              {
                "expr": "aim_cache_hit_rate"
              }
            ]
          }
        ]
      }
    }
```

---

## ü§ù **Contributing**

### **Development Setup**

```bash
# Clone repository
git clone https://github.com/amd/aim-engine.git
cd aim-engine

# Install dependencies
pip install -r requirements.txt

# Run tests
python -m pytest tests/

# Build container
docker build -f Dockerfile.aim-vllm -t aim-vllm:latest .
```

### **Adding New Models**

1. **Create model configuration**:
   ```yaml
   # models/new-model.yaml
   model_id: org/new-model
   name: New Model
   size: 13B
   family: Custom
   readiness_level: experimental
   ```

2. **Create recipe configuration**:
   ```yaml
   # recipes/new-model-mi300x.yaml
   recipe_id: new-model-mi300x-bf16
   model_id: org/new-model
   hardware: MI300X
   precision: bf16
   vllm_serve:
     2_gpu:
       enabled: true
       args:
         --model: org/new-model
         --dtype: bfloat16
   ```

3. **Test deployment**:
   ```bash
   aim-generate org/new-model --gpu-count 2
   ```

---

## üìÑ **License**

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

---

## üìû **Support**

- **Documentation**: [docs/](docs/)
- **Issues**: [GitHub Issues](https://github.com/amd/aim-engine/issues)
- **Discussions**: [GitHub Discussions](https://github.com/amd/aim-engine/discussions)
- **Email**: aim-engine-support@amd.com

---

*Last updated: January 2024*
*Version: 1.0.0* 
