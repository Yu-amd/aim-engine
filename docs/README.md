# AIM Engine - Complete Documentation

## ğŸ¯ **Overview**

AIM Engine is an intelligent AI model deployment system that automatically selects optimal configurations and manages inference endpoints using Docker containers. It combines the power of vLLM ROCm with intelligent orchestration tools.

## ğŸš€ **Quick Start**

### **Installation**
```bash
# Clone the repository
git clone <repository-url>
cd aim-engine

# Install dependencies
./install.sh

# Or install manually
pip install -e .
```

### **Basic Usage**
```bash
# Launch model with auto-detection
aim-engine launch Qwen/Qwen3-32B

# Launch with specific configuration
aim-engine launch Qwen/Qwen3-32B 4 --precision bf16 --backend vllm

# List running endpoints
aim-engine list

# Stop an endpoint
aim-engine stop aim-engine-qwen-qwen3-32b-4gpu-bf16-vllm
```

## ğŸ“š **Documentation Structure**

### **Guides**
- [Installation Guide](guides/installation.md) - Setup and installation instructions
- [User Guide](guides/user-guide.md) - How to use AIM Engine
- [Recipe Selection Guide](guides/recipe-selection.md) - Understanding recipe selection
- [Container Management](guides/container-management.md) - Working with containers

### **Architecture**
- [System Architecture](architecture/system-architecture.md) - Overall system design
- [Workflow](architecture/workflow.md) - Complete deployment workflow
- [Unified Container](architecture/unified-container.md) - Unified container approach
- [Production Deployment](architecture/production.md) - Production deployment guide

### **Examples**
- [Basic Examples](examples/basic-usage.md) - Simple usage examples
- [Advanced Examples](examples/advanced-usage.md) - Complex deployment scenarios
- [Troubleshooting](examples/troubleshooting.md) - Common issues and solutions

## ğŸ”§ **Key Features**

### **1. Intelligent Auto-Detection**
- **GPU Detection**: Automatically detects available GPUs
- **Optimal Selection**: Chooses best configuration based on model size
- **Smart Fallback**: Tries alternatives if primary choice fails

### **2. Model-Specific Recipe Loading**
- **Efficient Loading**: Only loads recipes for the target model
- **Fast Startup**: Optimized for quick deployment
- **Memory Efficient**: Minimal memory footprint

### **3. Single Container Deployment**
- **Unified Container**: Single container with AIM Engine and vLLM
- **Subprocess Execution**: vLLM runs as child process for efficiency
- **Shared Resources**: All components share GPU access and memory

### **4. Production Ready**
- **Health Checks**: Comprehensive endpoint validation
- **Resource Management**: Efficient GPU and memory usage
- **Monitoring**: Built-in metrics and logging
- **Scalability**: Support for multiple concurrent models

## ğŸ—ï¸ **System Architecture**


### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AIM Engine Container                     â”‚
â”‚   (aim-engine:latest - Unified vLLM + AIM Engine)           â”‚
â”‚                                                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚                 AIM Engine Core                         â”‚ â”‚
â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚ â”‚  â”‚   Launcher  â”‚  â”‚   Recipe    â”‚  â”‚     Cache       â”‚  â”‚ â”‚
â”‚ â”‚  â”‚             â”‚  â”‚  Selector   â”‚  â”‚    Manager      â”‚  â”‚ â”‚
â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              Process Management                        â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚   Config    â”‚  â”‚   Docker    â”‚  â”‚    Endpoint     â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ Generator   â”‚  â”‚   Manager   â”‚  â”‚    Manager      â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              Model Serving Process                     â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚ â”‚
â”‚  â”‚  â”‚     vLLM        â”‚  â”‚    SGLang       â”‚              â”‚ â”‚
â”‚  â”‚  â”‚   (Subprocess)  â”‚  â”‚   (Subprocess)  â”‚              â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AMD Hardware                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   MI250 GPU     â”‚  â”‚   MI300X GPU    â”‚  â”‚   MI325X GPU â”‚ â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Container Components

**Base Image**: `rocm/vllm:latest`
- **vLLM Runtime**: High-performance inference engine
- **ROCm Support**: AMD GPU acceleration
- **Python Environment**: PyTorch, transformers, and ML libraries

**Added Components**:
- **AIM Engine Tools**: Orchestration and management modules
- **Docker CLI**: Container management capabilities
- **Cache System**: Model caching and storage
- **Process Management**: Subprocess execution for model serving

### Key Design Principles

1. **Unified Container**: Single container with all components
2. **Subprocess Execution**: vLLM/SGLang run as child processes
3. **Shared Resources**: All components share GPU access and memory
4. **Docker CLI Available**: For container management if needed
5. **Efficient Deployment**: No Docker-in-Docker overhead


## ğŸ”„ **Complete Workflow**

### **Phase 1: Input Processing**
- Parse user command and validate inputs
- Initialize AIM Engine components

### **Phase 2: Auto-Detection & Resource Analysis**
- Detect available GPUs using `rocm-smi`
- Auto-select optimal GPU count based on model size
- Auto-select optimal precision based on model characteristics

### **Phase 3: Recipe Selection & Loading**
- Load ONLY recipes for the specific model
- Filter by precision, backend, GPU count
- Select best matching recipe

### **Phase 4: Configuration Generation**
- Generate Docker command arguments from recipe
- Create environment variables and volume mounts
- Configure networking and port mapping

### **Phase 5: Docker Container Management**
- Generate unique container name
- Pull `rocm/vllm:latest` image if needed
- Launch container with GPU access and configuration

### **Phase 6: Endpoint Readiness & Health Checks**
- Wait for container to start and vLLM to initialize
- Perform health checks on endpoint
- Verify model loading and GPU memory allocation

### **Phase 7: Validation & Testing**
- Send test inference request
- Verify response quality and performance
- Validate endpoint is ready for production use

### **Phase 8: Success Response & Monitoring**
- Return deployment information
- Setup monitoring and health check endpoints
- Provide usage instructions

## ğŸ³ **Container Deployment Models**

### **Model 1: Unified Container (Recommended)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AIM Engine Unified Container             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   AIM Engine    â”‚  â”‚   vLLM ROCm     â”‚  â”‚   Docker    â”‚ â”‚
â”‚  â”‚ Orchestration   â”‚  â”‚   Base Image    â”‚  â”‚   CLI       â”‚ â”‚
â”‚  â”‚   Tools         â”‚  â”‚                 â”‚  â”‚             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- Single container deployment
- Self-contained environment
- Dual mode operation (orchestration + direct serving)
- Easy distribution and deployment

### **Model 2: Separate Containers**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AIM Engine     â”‚    â”‚  vLLM Container â”‚
â”‚  Container      â”‚    â”‚                 â”‚
â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Orchestration â”‚    â”‚ â€¢ Model Serving â”‚
â”‚ â€¢ Recipe Mgmt   â”‚    â”‚ â€¢ GPU Access    â”‚
â”‚ â€¢ Docker Mgmt   â”‚    â”‚ â€¢ Inference API â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- Resource isolation
- Independent scaling
- Better for multi-node deployment
- Microservices architecture

## ğŸ“Š **Performance Benefits**

### **Memory Efficiency**
- **Before**: Loads all recipes (could be hundreds)
- **After**: Loads only model-specific recipes (typically 1-5)

### **Startup Time**
- **Before**: ~100-500ms to load all recipes
- **After**: ~10-50ms to load model-specific recipes

### **Scalability**
- **Before**: Performance degrades with recipe count
- **After**: Consistent performance regardless of total recipe count

## ğŸ¯ **Usage Examples**

### **Example 1: Full Auto-Detection**
```bash
# Launch model with complete auto-detection
aim-engine launch Qwen/Qwen3-32B

# What happens:
# 1. Detects available GPUs (e.g., 4 GPUs)
# 2. Auto-selects optimal GPU count (32B â†’ 4 GPUs)
# 3. Auto-selects optimal precision (32B â†’ bf16)
# 4. Loads only Qwen/Qwen3-32B recipes
# 5. Selects best matching recipe
# 6. Deploys with optimal configuration
```

### **Example 2: Customer Specified Configuration**
```bash
# Launch with specific GPU count and precision
aim-engine launch Qwen/Qwen3-32B 4 --precision bf16

# What happens:
# 1. Uses customer specified GPU count (4 GPUs)
# 2. Uses customer specified precision (bf16)
# 3. Loads only Qwen/Qwen3-32B recipes
# 4. Selects best matching recipe for 4 GPUs + bf16
# 5. Deploys with customer configuration
```

### **Example 3: Unified Container Deployment**
```bash

## ğŸ” **Configuration Selection Logic**

### **GPU Count Selection Priority**
1. **Customer specified** (if within available GPUs)
2. **Model size heuristic**:
   - 7B/8B models: 1 GPU
   - 13B/14B models: 2 GPUs
   - 32B/34B models: 4 GPUs
   - 70B/72B models: 8 GPUs
3. **Maximum available** (if heuristic exceeds available)

### **Precision Selection Priority**
1. **Customer specified**
2. **Model size heuristic**:
   - 7B/8B models: fp16 (faster, sufficient accuracy)
   - 13B+ models: bf16 (better numerical stability)
3. **Fallback alternatives** (if primary choice fails)

## ğŸ› ï¸ **Development**

### **Project Structure**
```
aim-engine/
â”œâ”€â”€ aim_*.py              # Core AIM Engine modules
â”œâ”€â”€ models/               # Model definitions
â”œâ”€â”€ recipes/              # AIM recipes
â”œâ”€â”€ templates/            # Configuration templates
â”œâ”€â”€ tests/                # Test files
â”œâ”€â”€ scripts/              # Utility scripts
â”œâ”€â”€ docs/                 # Documentation
â”œâ”€â”€ Dockerfile            # Standard container
â”œâ”€â”€ Dockerfile.unified    # Unified container
â””â”€â”€ requirements.txt      # Python dependencies
```

### **Running Tests**
```bash
# Run all tests
python -m pytest tests/

# Run specific test
python tests/test_aim_implementation.py

# Run with coverage
python -m pytest tests/ --cov=.
```

### **Building Containers**
```bash
# Build standard container
docker build -t aim-engine:latest .

### **For Users**
- âœ… **Simple**: Just specify the model, everything else is automatic
- âœ… **Fast**: Complete deployment in minutes
- âœ… **Reliable**: Comprehensive validation and testing
- âœ… **Flexible**: Override any auto-selected option

### **For System Administrators**
- âœ… **Resource Efficient**: Uses available hardware optimally
- âœ… **Scalable**: Performance doesn't degrade with more models
- âœ… **Monitorable**: Detailed logging and health checks
- âœ… **Maintainable**: Clean, modular architecture

### **For Developers**
- âœ… **Extensible**: Easy to add new features and optimizations
- âœ… **Testable**: Each component can be tested independently
- âœ… **Documented**: Clear workflow and API documentation
- âœ… **Standards Compliant**: Follows best practices

## ğŸ“ **Support**

For questions, issues, or contributions:
- Check the [troubleshooting guide](examples/troubleshooting.md)
- Review the [examples](examples/) for common use cases
- Open an issue on the project repository

---

**AIM Engine** - Intelligent AI Model Deployment Made Simple! ğŸš€ 
