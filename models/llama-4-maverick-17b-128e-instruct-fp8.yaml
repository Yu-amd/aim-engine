model_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
name: LLaMA 4 Maverick 17B 128E Instruct FP8
builder: Meta AI
family: Llama
size: 17B
huggingface_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
description: 'LLaMA 4 Maverick is a 17B parameter model with FP8 precision and 128K
  context length.  It''s designed for high-performance inference with reduced memory
  usage through FP8 quantization  while maintaining excellent instruction-following
  capabilities and extended context processing.

  '
logo: model_llama4_maverick.png
readiness_level: Production-Ready
status_badges:
- FP8
- FlashAttention
- Featured
tags:
- Text Generation
- Multilingual
- Instruction-Tuned
- vLLM-Compatible
- sglang-Compatible
license: Meta RAIL
endpoint: https://api.inference-hub.com/v1/chat/completions
demo_assets:
  notebook: https://github.com/inference-hub/notebooks/llama-4-maverick-17b-demo.ipynb
  demo_link: https://playground.inference-hub.com/models/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
aim_recipes:
- name: MI300X FP8
  hardware: MI300X
  precision: fp8
  recipe_file: configs/llama-4-maverick-17b-mi300x-fp8.yaml
- name: MI250 FP8
  hardware: MI250
  precision: fp8
  recipe_file: configs/llama-4-maverick-17b-mi250-fp8.yaml
api_examples:
  python: "import requests\n\nheaders = {\n    \"Authorization\": \"Bearer YOUR_API_KEY\"\
    ,\n    \"Content-Type\": \"application/json\"\n}\n\npayload = {\n    \"model\"\
    : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n    \"messages\": [{\"\
    role\": \"user\", \"content\": \"Hello\"}],\n    \"stream\": False\n}\n\nresponse\
    \ = requests.post(\"https://api.inference-hub.com/v1/chat/completions\", headers=headers,\
    \ json=payload)\nprint(response.json())\n"
  shell: "curl -X POST https://api.inference-hub.com/v1/chat/completions \\\n  -H\
    \ \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\"\
    \ \\\n  -d '{\n    \"model\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\
    ,\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n    \"stream\"\
    : false\n  }'\n"
  go: "package main\n\nimport (\n    \"bytes\"\n    \"fmt\"\n    \"io/ioutil\"\n \
    \   \"net/http\"\n)\n\nfunc main() {\n    jsonStr := []byte(`{\n        \"model\"\
    : \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n        \"messages\"\
    : [{\"role\": \"user\", \"content\": \"Hello\"}],\n        \"stream\": false\n\
    \    }`)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.inference-hub.com/v1/chat/completions\"\
    , bytes.NewBuffer(jsonStr))\n    req.Header.Set(\"Authorization\", \"Bearer YOUR_API_KEY\"\
    )\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client :=\
    \ &http.Client{}\n    resp, _ := client.Do(req)\n    body, _ := ioutil.ReadAll(resp.Body)\n\
    \    fmt.Println(string(body))\n}\n"
  typescript: "const response = await fetch(\"https://api.inference-hub.com/v1/chat/completions\"\
    , {\n  method: \"POST\",\n  headers: {\n    \"Authorization\": \"Bearer YOUR_API_KEY\"\
    ,\n    \"Content-Type\": \"application/json\"\n  },\n  body: JSON.stringify({\n\
    \    model: \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n    messages:\
    \ [{ role: \"user\", content: \"Hello\" }],\n    stream: false\n  })\n});\n\n\
    const data = await response.json();\nconsole.log(data.choices[0].message.content);\n"
  rust: "use axum::{\n    extract::Json,\n    http::StatusCode,\n    response::sse::{Event,\
    \ Sse},\n    routing::post,\n    Router,\n};\nuse serde::{Deserialize, Serialize};\n\
    use std::convert::Infallible;\nuse tokio_stream::wrappers::ReceiverStream;\n\n\
    #[derive(Deserialize)]\nstruct ChatRequest {\n    model: String,\n    messages:\
    \ Vec<Message>,\n    temperature: f32,\n    max_tokens: u32,\n    top_p: f32,\n\
    \    stream: bool,\n}\n\n#[derive(Serialize, Deserialize)]\nstruct Message {\n\
    \    role: String,\n    content: String,\n}\n\nasync fn chat_completion(Json(payload):\
    \ Json<ChatRequest>) -> Sse<impl Stream<Item = Result<Event, Infallible>>> {\n\
    \    let (tx, rx) = tokio::sync::mpsc::channel(100);\n    \n    tokio::spawn(async\
    \ move {\n        // Simulate streaming response\n        let response = format!(\"\
    Response for model: {}\", payload.model);\n        for chunk in response.chars()\
    \ {\n            let event = Event::default().data(chunk.to_string());\n     \
    \       let _ = tx.send(Ok(event)).await;\n            tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;\n\
    \        }\n    });\n    \n    Sse::new(ReceiverStream::new(rx))\n}\n\n#[tokio::main]\n\
    async fn main() {\n    let app = Router::new()\n        .route(\"/chat/completions\"\
    , post(chat_completion));\n    \n    axum::Server::bind(&\"0.0.0.0:3000\".parse().unwrap())\n\
    \        .serve(app.into_make_service())\n        .await\n        .unwrap();\n\
    }"
model_card:
  overview: >
    LLaMA 4 Maverick is a 17B parameter instruction-tuned language model with FP8 precision and 128K 
    context length. It's designed for high-performance inference with reduced memory usage through FP8 
    quantization while maintaining excellent instruction-following capabilities and extended context 
    processing. The model leverages Meta's latest training techniques and is optimized for efficient 
    deployment on modern hardware, particularly AMD GPUs. Maverick represents a significant advancement 
    in the Llama family, offering improved performance and efficiency compared to previous generations.

  intended_use:
    - Conversational AI and virtual assistants
    - Long-context document processing and analysis
    - Code generation and software development assistance
    - Content creation and creative writing
    - Research and educational applications
    - Multilingual text processing and translation
    - Question answering and information retrieval

  limitations:
    - May generate biased or harmful content despite safety measures
    - Performance may vary across different domains and tasks
    - Requires proper hardware optimization for optimal performance
    - Not suitable for safety-critical applications without additional safeguards
    - May hallucinate facts or generate inaccurate information
    - Limited to supported languages for optimal performance
    - Context window limitations may affect very long document processing

  training_data: >
    LLaMA 4 Maverick was trained on a diverse corpus of high-quality text data, including web content, 
    books, academic papers, and code repositories. The model underwent instruction tuning using 
    supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with 
    human preferences for helpfulness and safety. The training data includes multilingual content 
    and is filtered for quality and safety.

  evaluation:
    - MMLU (5-shot): ~75-80 (estimated based on model size)
    - HumanEval (0-shot): ~70-75 (estimated)
    - GSM-8K (CoT, 8-shot): ~85-90 (estimated)
    - MATH (CoT, 0-shot): ~50-60 (estimated)
    - Long-context evaluation: Optimized for 128K context
    - Multilingual performance: Strong across supported languages

  known_issues:
    - May produce biased content reflecting training data biases
    - Performance degradation on very long contexts beyond 128K tokens
    - Slower inference without proper hardware optimization
    - May struggle with highly specialized or technical domains
    - Limited reasoning capabilities on complex multi-step problems
    - FP8 quantization may introduce minor precision trade-offs

  references:
    - https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
    - https://github.com/meta-llama/llama
    - https://llama.meta.com/
    - https://arxiv.org/abs/2402.19155
