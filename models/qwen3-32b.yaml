model_id: Qwen/Qwen3-32B
name: Qwen3 32B
builder: Alibaba Qwen Team
family: Qwen3
size: 32.8B
huggingface_id: Qwen/Qwen3-32B
description: 'Qwen3-32B is the latest generation of large language models in the Qwen
  series, offering groundbreaking  advancements in reasoning, instruction-following,
  agent capabilities, and multilingual support.  It uniquely supports seamless switching
  between thinking mode (for complex logical reasoning, math, and coding)  and non-thinking
  mode (for efficient, general-purpose dialogue) within a single model.

  '
logo: model_qwen3_32b.png
readiness_level: Production-Ready
status_badges:
- BF16
- FlashAttention
- Featured
tags:
- Text Generation
- Reasoning
- Code Generation
- Multilingual
- Instruction-Tuned
- vLLM-Compatible
- sglang-Compatible
license: Apache 2.0
endpoint: https://api.inference-hub.com/v1/chat/completions
demo_assets:
  notebook: https://github.com/inference-hub/notebooks/qwen3-32b-demo.ipynb
  demo_link: https://playground.inference-hub.com/models/Qwen/Qwen3-32B
aim_recipes:
- name: MI300X BF16
  hardware: MI300X
  precision: bf16
  recipe_file: configs/qwen3-32b-mi300x-bf16.yaml
- name: MI250 BF16
  hardware: MI250
  precision: bf16
  recipe_file: configs/qwen3-32b-mi250-bf16.yaml
api_examples:
  python: "import requests\n\nheaders = {\n    \"Authorization\": \"Bearer YOUR_API_KEY\"\
    ,\n    \"Content-Type\": \"application/json\"\n}\n\npayload = {\n    \"model\"\
    : \"Qwen/Qwen3-32B\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"\
    Hello\"}],\n    \"stream\": False\n}\n\nresponse = requests.post(\"https://api.inference-hub.com/v1/chat/completions\"\
    , headers=headers, json=payload)\nprint(response.json())\n"
  shell: "curl -X POST https://api.inference-hub.com/v1/chat/completions \\\n  -H\
    \ \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\"\
    \ \\\n  -d '{\n    \"model\": \"Qwen/Qwen3-32B\",\n    \"messages\": [{\"role\"\
    : \"user\", \"content\": \"Hello\"}],\n    \"stream\": false\n  }'\n"
  go: "package main\n\nimport (\n    \"bytes\"\n    \"fmt\"\n    \"io/ioutil\"\n \
    \   \"net/http\"\n)\n\nfunc main() {\n    jsonStr := []byte(`{\n        \"model\"\
    : \"Qwen/Qwen3-32B\",\n        \"messages\": [{\"role\": \"user\", \"content\"\
    : \"Hello\"}],\n        \"stream\": false\n    }`)\n\n    req, _ := http.NewRequest(\"\
    POST\", \"https://api.inference-hub.com/v1/chat/completions\", bytes.NewBuffer(jsonStr))\n\
    \    req.Header.Set(\"Authorization\", \"Bearer YOUR_API_KEY\")\n    req.Header.Set(\"\
    Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp,\
    \ _ := client.Do(req)\n    body, _ := ioutil.ReadAll(resp.Body)\n    fmt.Println(string(body))\n\
    }\n"
  typescript: "const response = await fetch(\"https://api.inference-hub.com/v1/chat/completions\"\
    , {\n  method: \"POST\",\n  headers: {\n    \"Authorization\": \"Bearer YOUR_API_KEY\"\
    ,\n    \"Content-Type\": \"application/json\"\n  },\n  body: JSON.stringify({\n\
    \    model: \"Qwen/Qwen3-32B\",\n    messages: [{ role: \"user\", content: \"\
    Hello\" }],\n    stream: false\n  })\n});\n\nconst data = await response.json();\n\
    console.log(data.choices[0].message.content);\n"
  rust: "use axum::{\n    extract::Json,\n    http::StatusCode,\n    response::sse::{Event,\
    \ Sse},\n    routing::post,\n    Router,\n};\nuse serde::{Deserialize, Serialize};\n\
    use std::convert::Infallible;\nuse tokio_stream::wrappers::ReceiverStream;\n\n\
    #[derive(Deserialize)]\nstruct ChatRequest {\n    model: String,\n    messages:\
    \ Vec<Message>,\n    temperature: f32,\n    max_tokens: u32,\n    top_p: f32,\n\
    \    stream: bool,\n}\n\n#[derive(Serialize, Deserialize)]\nstruct Message {\n\
    \    role: String,\n    content: String,\n}\n\nasync fn chat_completion(Json(payload):\
    \ Json<ChatRequest>) -> Sse<impl Stream<Item = Result<Event, Infallible>>> {\n\
    \    let (tx, rx) = tokio::sync::mpsc::channel(100);\n    \n    tokio::spawn(async\
    \ move {\n        // Simulate streaming response\n        let response = format!(\"\
    Response for model: {}\", payload.model);\n        for chunk in response.chars()\
    \ {\n            let event = Event::default().data(chunk.to_string());\n     \
    \       let _ = tx.send(Ok(event)).await;\n            tokio::time::sleep(tokio::time::Duration::from_millis(50)).await;\n\
    \        }\n    });\n    \n    Sse::new(ReceiverStream::new(rx))\n}\n\n#[tokio::main]\n\
    async fn main() {\n    let app = Router::new()\n        .route(\"/chat/completions\"\
    , post(chat_completion));\n    \n    axum::Server::bind(&\"0.0.0.0:3000\".parse().unwrap())\n\
    \        .serve(app.into_make_service())\n        .await\n        .unwrap();\n\
    }"
model_card:
  overview: 'library_name: transformers license_link: https://huggingface.co/Qwen/Qwen3-32B/blob/main/LICENSE
    pipeline_tag: text-generation'
  intended_use:
  - Text-Generation tasks
  limitations:
  - May generate biased or harmful content
  - Not suitable for safety-critical applications
  - Performance may vary across different tasks and domains
  training_data: Training data information not specified in model card.
  evaluation:
  - Evaluation metrics not specified in model card
  known_issues:
  - May produce biased content
  - Limited reasoning capabilities
  - Performance varies across languages and domains
  references:
  - https://arxiv.org/abs/2505.09388},
  - https://huggingface.co/Qwen/Qwen3-32B
